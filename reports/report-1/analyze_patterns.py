"""
Analyze patterns in raw vs. cleaned text files.

This script compares raw and cleaned text files to identify patterns
and changes introduced by the cleaning process.

Note: This script is generated by Cursor Agent using Claude 3.5 Sonnet
and is not my own work. It's used for demonstration purposes.

"""

import os
import re
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import numpy as np

def load_text(file_path):
    """Load text from a file."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

def tokenize_words(text):
    """Simple word tokenization using regex."""
    return re.findall(r'\b\w+\b', text.lower())

def tokenize_sentences(text):
    """Simple sentence tokenization using regex."""
    return re.split(r'[.!?]+', text)

def analyze_text(text, name):
    """
    Analyze text and return various metrics.
    
    Args:
        text: The text to analyze
        name: Name of the text source
        
    Returns:
        Dictionary containing various metrics
    """
    # Basic metrics
    char_count = len(text)
    words = tokenize_words(text)
    word_count = len(words)
    sent_count = len(tokenize_sentences(text))
    
    # Word length distribution
    word_lengths = [len(word) for word in words]
    avg_word_length = sum(word_lengths) / len(word_lengths) if word_lengths else 0
    
    # Vocabulary metrics
    vocab = set(words)
    vocab_size = len(vocab)
    
    # Most common words
    word_freq = Counter(words)
    most_common = word_freq.most_common(10)
    
    # Special character analysis
    special_chars = re.findall(r'[^\w\s]', text)
    special_char_freq = Counter(special_chars)
    
    # Whitespace analysis
    whitespace_count = len(re.findall(r'\s', text))
    newline_count = len(re.findall(r'\n', text))
    
    # HTML/PDF artifacts (if any)
    html_tags = len(re.findall(r'<[^>]+>', text))
    page_numbers = len(re.findall(r'Page \d+', text))
    
    return {
        'name': name,
        'char_count': char_count,
        'word_count': word_count,
        'sent_count': sent_count,
        'avg_word_length': avg_word_length,
        'vocab_size': vocab_size,
        'most_common_words': most_common,
        'special_char_freq': special_char_freq,
        'whitespace_count': whitespace_count,
        'newline_count': newline_count,
        'html_tags': html_tags,
        'page_numbers': page_numbers
    }

def compare_texts(raw_text, clean_text, source_name):
    """
    Compare raw and cleaned texts and print differences.
    
    Args:
        raw_text: The raw text
        clean_text: The cleaned text
        source_name: Name of the text source (e.g., 'website', 'pdf')
    """
    raw_metrics = analyze_text(raw_text, f"Raw {source_name}")
    clean_metrics = analyze_text(clean_text, f"Clean {source_name}")
    
    print(f"\nAnalysis for {source_name} data:")
    print("-" * 50)
    
    # Compare basic metrics
    print("\nBasic Metrics:")
    print(f"{'Metric':<20} {'Raw':<15} {'Clean':<15} {'Change':<15}")
    print("-" * 65)
    
    metrics_to_compare = [
        ('Character Count', 'char_count'),
        ('Word Count', 'word_count'),
        ('Sentence Count', 'sent_count'),
        ('Average Word Length', 'avg_word_length'),
        ('Vocabulary Size', 'vocab_size'),
        ('Whitespace Count', 'whitespace_count'),
        ('Newline Count', 'newline_count'),
        ('HTML Tags', 'html_tags'),
        ('Page Numbers', 'page_numbers')
    ]
    
    for metric_name, metric_key in metrics_to_compare:
        raw_value = raw_metrics[metric_key]
        clean_value = clean_metrics[metric_key]
        change = clean_value - raw_value
        change_str = f"{change:+.2f}" if isinstance(change, float) else f"{change:+d}"
        
        print(f"{metric_name:<20} {raw_value:<15.2f} {clean_value:<15.2f} {change_str:<15}")
    
    # Compare most common words
    print("\nMost Common Words (Raw):")
    for word, count in raw_metrics['most_common_words']:
        print(f"  {word}: {count}")
    
    print("\nMost Common Words (Clean):")
    for word, count in clean_metrics['most_common_words']:
        print(f"  {word}: {count}")
    
    # Compare special characters
    print("\nSpecial Characters (Raw):")
    for char, count in raw_metrics['special_char_freq'].most_common(10):
        print(f"  {repr(char)}: {count}")
    
    print("\nSpecial Characters (Clean):")
    for char, count in clean_metrics['special_char_freq'].most_common(10):
        print(f"  {repr(char)}: {count}")

def main():
    # Get script directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Analyze website data
    raw_web_path = os.path.join(script_dir, "raw_website_data.txt")
    clean_web_path = os.path.join(script_dir, "clean_website_data.txt")
    
    if os.path.exists(raw_web_path) and os.path.exists(clean_web_path):
        raw_web_text = load_text(raw_web_path)
        clean_web_text = load_text(clean_web_path)
        compare_texts(raw_web_text, clean_web_text, "Website")
    else:
        print("Website data files not found")
    
    # Analyze PDF data
    raw_pdf_path = os.path.join(script_dir, "raw_pdf_data.txt")
    clean_pdf_path = os.path.join(script_dir, "clean_pdf_data.txt")
    
    if os.path.exists(raw_pdf_path) and os.path.exists(clean_pdf_path):
        raw_pdf_text = load_text(raw_pdf_path)
        clean_pdf_text = load_text(clean_pdf_path)
        compare_texts(raw_pdf_text, clean_pdf_text, "PDF")
    else:
        print("PDF data files not found")

if __name__ == "__main__":
    main() 