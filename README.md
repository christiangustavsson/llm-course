# Understanding and Building LLMs



## Training Corpus
OpenWebText Corpus is an open-source replication of OpenAI's WebText corpus, which was used to train GPT-2, created by Aaron Gokaslan and Vanya Cohen (2019). It consists of web content extracted from URLs shared on Reddit, filtered to include only those with a high karma score, ensuring relevance and quality. 

Original work: http://Skylion007.github.io/OpenWebTextCorpus
Roughly one Quarter of OpenWebText: https://www.kaggle.com/datasets/isamuisozaki/roughly-one-quarter-of-openwebtext/data