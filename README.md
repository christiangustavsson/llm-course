# Understanding and Building LLMs



## Training Corpus
OpenWebText Corpus is an open-source replication of OpenAI's WebText corpus, which was used to train GPT-2, created by Aaron Gokaslan and Vanya Cohen (2019). It consists of web content extracted from URLs shared on Reddit, filtered to include only those with a high karma score, ensuring relevance and quality. 

The OpenWebText Corpus is downloadable from [Huggingface](https://huggingface.co/datasets/Skylion007/openwebtext). The re-work limiting the data size to roughly one quarter kan be downloaded from [Kaggle](https://www.kaggle.com/datasets/isamuisozaki/roughly-one-quarter-of-openwebtext/data).


